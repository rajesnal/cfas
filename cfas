#!/usr/bin/env python
"""
Count files and sizes on linux faster than anyone else.

This is created to identify large chunks of data and files hidden in subfolders on
large +10TB storage drives, where a normal 'find' or 'du' either takes forever or
does not identify the wanted hotspots.

It is Python, but uses libc to increase the handling of folders with too many files for other tools.


Main features:
 - Supports a progress option, which outputs a status for every S seconds. Very useful for folders with >100.000 files.
 - Filters output based on a minimum of filecount and/or filesize. Great for identifying hotspots with many >100.000 files.
 - Performs user separated counting in one go, thus reducing the required execution time.
 - Counts hard-linked files once.
 - Output format is very parser friendly.
 - Can perform counting at any depth level.

Author: Rune Moellegaard Friborg <runef@birc.au.dk>
"""


import getopt
import sys
import multiprocessing
import threading, time, Queue
import os
import os.path
import stat
import platform
import re
import numbers
from copy import copy
import pwd


from ctypes import CDLL, c_char_p, c_uint, c_int, c_long, c_longlong, c_ushort, c_byte, c_char, Structure, POINTER
from ctypes.util import find_library

##############################################################
####################### Configuration ########################

VERSION="0.06"
UPDATED="2014-10-03"

units = { 'B': 2 ** 0,
          'K': 2 ** 10,
          'M': 2 ** 20,
          'G': 2 ** 30,
          'T': 2 ** 40,
          'P': 2 ** 50,
          'E': 2 ** 60,
          'Z': 2 ** 70,
          'Y': 2 ** 80 }

number_unit_pattern = r'^(?P<number>[0-9]*(\.[0-9]+)?)(?P<unit>[a-zA-Z]+)?$'
number_unit_matcher = re.compile(number_unit_pattern)

def supported_units():
  """Returns a list of valid unit suffixes.

  Unit suffixes are case-insensitive."""
  return units.keys()

def from_human(value, default_unit='M'):
  """Convert a file size in human format to bytes.

  Converts a string consisting of a floating point number and a unit suffix to
  bytes. If no unit suffix is found, default_unit is used. No whitespace is
  allowed in the string. Unit suffixes are case insensitive.

  E.g. to convert the string '512K' to bytes:

    >>> from_human('512K')
    524288
  """
  match = number_unit_matcher.match(value)
  if not match:
    raise ValueError('value must be a positive number followed by a valid unit')

  number, _, unit = match.groups()
  unit = unit or default_unit

  if not float(number) >= 0:
    raise ValueError('value must be a positive number followed by a valid unit')

  normalized_unit = unit.upper()
  if normalized_unit not in units.iterkeys():
    raise ValueError('unit is invalid')
  return int(float(number) * units[normalized_unit])

def to_human(value):
  """Convert a number of bytes to a human-readable string.

  Will convert any positive integer value to a human-readable string with a unit
  suffix such as K, M or G, depending on the size of the number. E.g.

    >>> to_human(1024)
    1.0K
    >>> to_human(524288)
    512.0K
    >>> to_human(78293719)
    74.7M
  """
  if not isinstance(value, numbers.Number):
    raise ValueError('first argument must be a positive integer')
  if not value >= 0:
    raise ValueError('first argument most be a positive integer')

  unit, multiplier = 'B', 1.0
  for curr_unit, curr_multiplier in units.iteritems():
    if value > curr_multiplier and \
       value < curr_multiplier * 1024:
      unit, multiplier = curr_unit, curr_multiplier
      break

  human_value = float(value) / multiplier
  return '{number:.1f}{unit}'.format(number=human_value, unit=unit)

class c_dir(Structure):
  """Opaque type for directory entries, corresponds to struct DIR"""
  pass
c_dir_p = POINTER(c_dir)

class c_dirent(Structure):
  if platform.system() == 'Darwin':
    _fields_ = [
      ('d_ino', c_long, 32), # inode number
      ('d_reclen', c_ushort, 16), # length of this record
      ('d_type', c_uint, 8), # type of file; not supported by all file system types
      ('d_namlen', c_ushort, 8), # type of file; not supported by all file system types
      ('d_name', c_char * 256) # filename
    ]
  else:
    _fields_ = [
      ('d_ino', c_long), # inode number
      ('d_off', c_long), # offset to the next dirent
      ('d_reclen', c_ushort), # length of this record
      ('d_type', c_byte), # type of file; not supported by all file system types
      ('d_name', c_char * 4096) # filename
    ]

c_dirent_p = POINTER(c_dirent)

c_lib = CDLL(find_library("c"))
opendir = c_lib.opendir
opendir.argtypes = [c_char_p]
opendir.restype = c_dir_p

# FIXME Should probably use readdir_r here
readdir = c_lib.readdir
readdir.argtypes = [c_dir_p]
readdir.restype = c_dirent_p

closedir = c_lib.closedir
closedir.argtypes = [c_dir_p]
closedir.restype = c_int

D_TYPE_DIR = 4

def read_dir_entries(path):
  dir_p = opendir(".")
  try:
    while True:
      p = readdir(dir_p)
      if not p:
        break
      yield p.contents.d_name, p.contents.d_type
  finally:
    closedir(dir_p)

inode_list = set()


Qsizes = []
Qfiles = []

template_user = '{0:>15} {1:>15} {2:>15} {3}'
template = '{0:>15} {1:>15} {2}'

# Get all uids
uidDict = {}
for p in pwd.getpwall():
  uidDict[p.pw_uid] = p.pw_name

# Unknown uids 
uidDict[None] = 'Unknown'


# Output status thread
class outputStateThread(threading.Thread):
  def __init__(self, seconds, q, workers):
    threading.Thread.__init__(self)
    self.seconds = seconds
    self.output = seconds > 0
    self.daemon = True
    self.workerJobsSubmitted = [0 for i in range(workers)]
    self.workerJobsDone = [0 for i in range(workers)]
    self.q = q

  def run(self):
    previousCount = 0
    previousSize = 0
    newCount = 0
    newSize = 0
    previousTime = time.time()

    # Add one submitted job, for the initial root directory
    self.workerJobsSubmitted[0] += 1

    while True:
      val = self.q.get()
      if len(val) == 2:
        # Process update
        submitted, pid = val
        self.workerJobsSubmitted[pid] += submitted
        self.workerJobsDone[pid] +=1
        
        # Check
        if sum(self.workerJobsSubmitted) == sum(self.workerJobsDone):
          sys.stderr.write(str(self.workerJobsSubmitted) + "\n")
          sys.stderr.write(str(self.workerJobsDone) + "\n")
          # Start shutdown
          sys.stderr.write("Initiating shutdown\n")
          for i in self.workerJobsDone:
            CountWorkerQueue.put((None, None, None, None, None))
          sys.stderr.write("Done\n")       

          # Done
          CountResultQueue.put((None, None, None, None, None, None))            

      else:
        cwd, count, size = val
        
        if self.output:
          newTime = time.time()
          newCount += count
          newSize += size
          if (newTime-previousTime > self.seconds):
            speed = (newCount-previousCount)/(newTime-previousTime)
            sys.stderr.write("# "+str(int(speed))+" files/s "+str(newCount)+" "+to_human(newSize)+"B: "+cwd+"\n")
            sys.stderr.flush()
        
            previousTime = newTime
            previousSize = newSize
            previousCount = newCount
          


# StatFileWorker
class statFileWorkerThread(threading.Thread):
  def __init__(self, cwd, q, files):
    threading.Thread.__init__(self)
    self.q = q
    self.cwd = cwd
    self.files = files
    self.daemon = True

  def run(self):
    stat(self.cwd, self.q, self.files)

def stat(cwd, q, files):
    for name, filetype in files:
      abs_path = os.path.join(cwd, name)
      st = os.lstat(abs_path)
      q.put((name,filetype,st))


CountWorkerQueue = multiprocessing.Queue()
CountResultQueue = multiprocessing.Queue()
StateQueue = multiprocessing.Queue()

class countWorkerProcess(multiprocessing.Process):
  def __init__(self, args, qIn, qOut, pid):
    multiprocessing.Process.__init__(self)
    self.args = args
    self.qIn = qIn
    self.qOut = qOut
    self.processindex = pid
    self.daemon = True

  def run(self):
    while True:
      path, name, parent_inode, inode, curr_depth = self.qIn.get()

      if path == None:
        # Quit
        break

      try:
        abs_path = os.path.join(path, name)
        os.chdir(abs_path)
        
        self.count(name, parent_inode, inode, curr_depth + 1)

      except OSError, e:
        if e.errno == 13:
          sys.stderr.write('error: access denied to directory ' + os.path.join(path, name) + '\n')
        elif e.errno == 2:
          sys.stderr.write('error: no such file or directory ' + os.path.join(path, name) + '\n')
        else:
          raise e


  def count(self, path, parent_inode, inode, curr_depth):

    stateCount = 0
    stateSize = 0
    file_count = {}
    file_size = {}

    Qfiles.append({})
    Qsizes.append({})

    cwd = os.getcwd()
    q = Queue.Queue()
    entry_generator = read_dir_entries(path)

    dirs = []  
    files = []

    for val in entry_generator:
      if val[0] == "." or val[0] == "..":
        continue
      if val[1] == D_TYPE_DIR:
        dirs.append(val)
        continue
      files.append(val)

    l = len(files)
    c = 2
    if l > 1000:
      split = int(l/c)
      for i in xrange(c-1):
        t = statFileWorkerThread(cwd, q, files[i*split:(i+1)*split])
        t.start()
      t = statFileWorkerThread(cwd, q, files[(c-1)*split:])
      t.start()
    else:
      stat(cwd, q, files)


    for i in xrange(len(files)):
      name, filetype, st = q.get()

      # If the inode wasn't visited before, we'll keep track of its
      # size. We save a lookup in inode_list by first asking if the
      # inode is linked to exactly once. If this is the case, it
      # cannot be in inode_list anyway.
      if st.st_nlink == 1 or st.st_ino not in inode_list:
        stateSize += st.st_size
        file_size[st.st_uid] = file_size.setdefault(st.st_uid, 0) + st.st_size

      # If inode is pointed to by more than one link, we keep track
      # of it so that we won't count the size of it more than once.
      if st.st_nlink > 1:
        inode_list.add(st.st_ino)

      # Always count up file_count.
      file_count[st.st_uid] = file_count.setdefault(st.st_uid, 0) + 1

      stateCount += 1
      # Send progress
      if (stateCount % 10000) == 0:
        StateQueue.put((cwd, stateCount, stateSize))
        stateCount = 0
        stateSize = 0

    # Send state
    StateQueue.put((len(dirs), self.processindex))
    
    if dirs:
      stat(cwd, q, dirs)

    l = len(dirs)
    for i in xrange(l):
      name, filetype, st = q.get()

      # Always count up file_count.
      file_count[st.st_uid] = file_count.setdefault(st.st_uid, 0) + 1

      stateCount += 1

      # Submit work
      CountWorkerQueue.put((cwd, name, inode, st.st_ino, curr_depth +1))

    # Send progress
    StateQueue.put((cwd, stateCount, stateSize))
      
    CountResultQueue.put((path, parent_inode, inode, curr_depth, file_count, file_size))


  
def main(args):

  workers = 8

  t = outputStateThread(args.status, StateQueue, workers)
  t.start()

  if not args.quiet:
    if args.user:
      print template_user.format('User', 'Files', 'Size', 'Path')
    else:
      print template.format('Files', 'Size', 'Path')

  root_dir = args.directory

  CountWorkerQueue.put(('/.', root_dir, None, None, 0))

  for i in xrange(workers):
    p = countWorkerProcess(args, CountWorkerQueue, CountResultQueue, i)
    p.start()

  ALL = {}
  while True:
    name, parent_inode, inode, curr_depth, file_count, file_size = CountResultQueue.get()
    print name
    ALL[inode] = (parent_inode, name, file_count, file_size)
    
    if name == None:
      sys.exit(0)

    

  #CountResultQueue.put((path, parent_inode, inode, curr_depth, file_count, file_size))


  ## CREATE COLLECTOR PROCESS START
  for i in xrange(1, min(len(Qfiles), args.recurse) + 1):
    for uid in file_count:
      Qfiles[-i][uid] = Qfiles[-i].setdefault(uid, 0) + file_count[uid]
    for uid in file_size:
      Qsizes[-i][uid] = Qsizes[-i].setdefault(uid, 0) + file_size[uid]

  file_count = Qfiles.pop()
  file_size = Qsizes.pop()

  if args.max_depth >= curr_depth:
    # output
    if args.user:
      # Split output on user
      for uid in file_count:
        if file_count[uid] > args.file_limit or file_size[uid] > args.size_limit:
          print template_user.format(uidDict.setdefault(uid, 'Unknown'),
                                     file_count[uid], 
                                     (to_human(file_size[uid]) if args.human_readable else file_size[uid]),
                                     os.getcwd())
      
    else:
      # Summarise output
      sum_file_count = sum(file_count.values())
      sum_file_size  = sum(file_size.values())
      if sum_file_count >= args.file_limit or sum_file_size >= args.size_limit:
        print template.format(sum_file_count, 
                              (to_human(sum_file_size) if args.human_readable else sum_file_size),
                              os.getcwd())

  ## CREATE COLLECTOR PROCESS END

##############################################################
######################### Help ###############################

def usage():
    print("""cfas version """+VERSION+""" by Rune M. Friborg (updated """+UPDATED+""")
Usage:
  cfas [-h] [--directory DIRECTORY] --file-limit N --size-limit K
            [--recurse LEVELS] [--quiet] [--user] [--human-readable] [--status S]

Optional arguments:
  -h, --help            show this help message and exit
  --directory DIRECTORY, -d DIRECTORY
                        the directory to walk
  --max-depth d, -D d   only about for folder level up to d.
                        -D 0 (without -r) summarizes all.
  --file-limit N, -n N  ouput directory if it contains at least N files
  --size-limit K, -k K  output directory if the total size of files is K
  
  --recurse LEVELS, -r LEVELS
                        the directory depth to summarise for every count.
                        -r 0 will count the files separately for every directory
  --quiet, -q           do not output header (usefull for output parsing)
  --user, -u
                        add user column and split counts between users
  --human-readable, -H
                        output numbers in human readable format
  --status S, -s S      output status to stderr for every S second
""")

  
class ArgContainer():
  def __init__(self):
    self.directory      = os.getcwd()
    self.file_limit     = 0
    self.size_limit     = 0
    self.recurse        = 1000
    self.max_depth      = 1000
    self.quiet          = False
    self.human_readable = False
    self.user           = False
    self.status         = 0


if __name__ == '__main__':
  #args = parser.parse_args()
  #args.file_limit = 
  #args.size_limit = int(from_human(args.size_limit, default_unit='B'))

  try:
    opts, _ = getopt.getopt(sys.argv[1:], "hd:D:n:k:r:qHup:s:", ["help", "directory=", "max-depth=", "file-limit=", "size-limit=", "recurse=", "quiet", "human-readable", "user", "progress=", "status="])
  except getopt.GetoptError, err:
    # print help information and exit:
    print str(err) # will print something like "option -a not recognized"
    sys.exit(2)

  args = ArgContainer()
  for o, a in opts:
    if a and a[0] == "=":
      a = a[1:]

    if o in ("-d", "--directory"):
      args.directory = a
    elif o in ("-D", "--max-depth"):
      args.max_depth = int(a)
    elif o in ("-n", "--file-limit"):
      args.file_limit = int(from_human(a, default_unit='B'))
    elif o in ("-k", "--size-limit"):
      args.size_limit = int(from_human(a, default_unit='B'))
    elif o in ("-r", "--recurse"):
      args.recurse = int(a)
    elif o in ("-q", "--quiet"):
      args.quiet = True
    elif o in ("-H", "--human-readable"):
      args.human_readable = True
    elif o in ("-u", "--user"):
      args.user = True
    elif o in ("-p", "--progress"):
      #ignore
      pass
    elif o in ("-s", "--status"):
      args.status = int(a)    
    elif o in ("-h", "--help"):
      usage()
      sys.exit()
    else:
      assert False, "unhandled option"

  args.recurse += 1
  if args.file_limit == 0 and args.size_limit > 0:
    args.file_limit = 10**18
  elif args.size_limit == 0 and args.file_limit > 0:
    args.size_limit = 10**18

  try:
    main(args)
  except KeyboardInterrupt:
    sys.exit(1)

